{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6ofEMYqo56ai2vwKXz6zW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugandhab31/NotesRAGDemoApp/blob/main/NotesRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_4UghyEYG3s4"
      },
      "outputs": [],
      "source": [
        "!pip install -U requests==2.32.5 --quiet\n",
        "!pip install -U \"langgraph>=0.2.26\"  --quiet\n",
        "!pip install langchain langchain-community sentence-transformers faiss-cpu transformers accelerate torch --quiet\n",
        "!python -m pip show langgraph\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "notes_db = [\n",
        "    {\"id\": 1, \"tags\": [\"AI\", \"LangGraph\"], \"text\": \"LangGraph lets you build agent workflows as graphs with memory and control flow.\"},\n",
        "    {\"id\": 2, \"tags\": [\"AI\"], \"text\": \"Retrieval-Augmented Generation (RAG) connects LLMs to external knowledge to reduce hallucinations.\"},\n",
        "    {\"id\": 3, \"tags\": [\"Python\"], \"text\": \"FastAPI is a fast, modern framework for building web APIs in Python.\"},\n",
        "    {\"id\": 4, \"tags\": [\"AI\", \"Embeddings\"], \"text\": \"Sentence-Transformers provide high-quality text embeddings for semantic search.\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "5wfBSnQmHvuF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize embedding model wrapper for LangChain\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "texts = [n[\"text\"] for n in notes_db]\n",
        "metas = [{\"tags\": n[\"tags\"], \"id\": n[\"id\"]} for n in notes_db]\n",
        "\n",
        "# Build FAISS vector store (LangChain handles embeddings internally)\n",
        "vectorstore = FAISS.from_texts(texts, embedding=embedding_model, metadatas=metas)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "QIcHZtzrIvJt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from typing import TypedDict\n",
        "\n",
        "class RAGState(TypedDict):\n",
        "    query: str\n",
        "    context: str\n",
        "    answer: str\n",
        "\n",
        "builder = StateGraph(state_schema=RAGState)\n",
        "\n",
        "def retriever_node(state):\n",
        "    docs = retriever.get_relevant_documents(state[\"query\"])\n",
        "    state[\"context\"] = \"\\n\".join([d.page_content for d in docs])\n",
        "    return state\n",
        "\n",
        "def summarizer_node(state):\n",
        "    prompt = f\"Answer concisely using only this context:\\n{state['context']}\\n\\nQuestion: {state['query']}\\nAnswer:\"\n",
        "    response = llm(prompt, max_new_tokens=150)[0][\"generated_text\"]\n",
        "    state[\"answer\"] = response\n",
        "    return state\n",
        "\n",
        "builder.add_node(\"retriever\", retriever_node)\n",
        "builder.add_node(\"summarizer\", summarizer_node)\n",
        "builder.add_edge(\"retriever\", \"summarizer\")\n",
        "builder.set_entry_point(\"retriever\")\n",
        "builder.set_finish_point(\"summarizer\")\n",
        "\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "app = builder.compile(checkpointer=checkpointer)\n"
      ],
      "metadata": {
        "id": "PBi8z2r-M8A8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import json\n",
        "\n",
        "# same embedding model you used for FAISS\n",
        "semantic_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "metrics = []\n",
        "\n",
        "def run_query(query, session_id=\"default\"):\n",
        "  \"\"\"Run a quey through RAG graph with session memory + metrics tracking\"\"\"\n",
        "  config = {\"configurable\":{\"thread_id\": session_id}}\n",
        "  state = app.invoke({\"query\": query, \"context\": \"\", \"answer\": \"\"}, config=config)\n",
        "\n",
        "  # Print human-readable response\n",
        "  print(f\"\\nSession: {session_id}\")\n",
        "  print(f\"Query: {query}\")\n",
        "  print(f\"Answer: {state['answer'][:250]}\\n\")\n",
        "\n",
        "  # completeness = round(min(1.0, len(state[\"context\"].split()) / (len(query.split()) * 5)), 2)\n",
        "  # faithfulness = 1.0 if any(w in state[\"answer\"].lower() for w in query.lower().split()[:2]) else 0.8\n",
        "  # mismatch = 1-faithfulness\n",
        "\n",
        "  query_emb = semantic_model.encode(query, convert_to_tensor=True)\n",
        "  answer_emb = semantic_model.encode(state[\"answer\"], convert_to_tensor=True)\n",
        "  context_emb = semantic_model.encode(state[\"context\"], convert_to_tensor=True)\n",
        "\n",
        "  # Faithfulness → how close the answer is to the retrieved context\n",
        "  faithfulness = float(util.cos_sim(answer_emb, context_emb))\n",
        "  # Retrieval relevance → how close the retrieved context is to the user’s query\n",
        "  retrieval_relevance = float(util.cos_sim(query_emb, context_emb))\n",
        "  # Completeness → estimated by context length ratio\n",
        "  completeness = round(min(1.0, len(state[\"context\"].split()) / (len(query.split()) * 5)), 2)\n",
        "\n",
        "  print(f\"Completeness: {completeness}\")\n",
        "  print(f\"Faithfulness: {faithfulness}\")\n",
        "  print(f\"Retrieval Relevence: {retrieval_relevance}\")\n",
        "\n",
        "  metrics.append({\n",
        "      \"session\": session_id,\n",
        "      \"query\": query,\n",
        "      \"completeness\": completeness,\n",
        "      \"faithfulness\": faithfulness,\n",
        "      \"Retrieval Relevence\": retrieval_relevance,\n",
        "      \"timestamp\": datetime.now().isoformat()\n",
        "  })\n",
        "\n",
        "  print(json.dumps(metrics))\n",
        "\n",
        "  return state\n"
      ],
      "metadata": {
        "id": "_N0b5Fp6Wrlh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_query(\"What is LangGraph?\", \"user001\")\n",
        "run_query(\"How does RAG reduce hallucinations?\", \"user001\")\n",
        "run_query(\"Explain FastAPI briefly\", \"user001\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sxygrKgYk_U",
        "outputId": "2ebb0da6-72cb-44e1-c943-8238131c29bb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Session: user001\n",
            "Query: What is LangGraph?\n",
            "Answer: LangGraph lets you build agent workflows as graphs with memory and control flow. Sentence-Transformers provide high-quality text embeddings for semantic search\n",
            "\n",
            "Completeness: 1.0\n",
            "Faithfulness: 0.996911883354187\n",
            "Retrieval Relevence: 0.4405069947242737\n",
            "[{\"session\": \"user001\", \"query\": \"What is LangGraph?\", \"completeness\": 1.0, \"faithfulness\": 0.996911883354187, \"Retrieval Relevence\": 0.4405069947242737, \"timestamp\": \"2025-11-08T09:32:08.087729\"}]\n",
            "\n",
            "Session: user001\n",
            "Query: How does RAG reduce hallucinations?\n",
            "Answer: connects LLMs to external knowledge\n",
            "\n",
            "Completeness: 0.96\n",
            "Faithfulness: 0.445770263671875\n",
            "Retrieval Relevence: 0.44643956422805786\n",
            "[{\"session\": \"user001\", \"query\": \"What is LangGraph?\", \"completeness\": 1.0, \"faithfulness\": 0.996911883354187, \"Retrieval Relevence\": 0.4405069947242737, \"timestamp\": \"2025-11-08T09:32:08.087729\"}, {\"session\": \"user001\", \"query\": \"How does RAG reduce hallucinations?\", \"completeness\": 0.96, \"faithfulness\": 0.445770263671875, \"Retrieval Relevence\": 0.44643956422805786, \"timestamp\": \"2025-11-08T09:32:10.949573\"}]\n",
            "\n",
            "Session: user001\n",
            "Query: Explain FastAPI briefly\n",
            "Answer: FastAPI is a fast, modern framework for building web APIs in Python. LangGraph lets you build agent workflows as graphs with memory and control flow.\n",
            "\n",
            "Completeness: 1.0\n",
            "Faithfulness: 1.0\n",
            "Retrieval Relevence: 0.5390328764915466\n",
            "[{\"session\": \"user001\", \"query\": \"What is LangGraph?\", \"completeness\": 1.0, \"faithfulness\": 0.996911883354187, \"Retrieval Relevence\": 0.4405069947242737, \"timestamp\": \"2025-11-08T09:32:08.087729\"}, {\"session\": \"user001\", \"query\": \"How does RAG reduce hallucinations?\", \"completeness\": 0.96, \"faithfulness\": 0.445770263671875, \"Retrieval Relevence\": 0.44643956422805786, \"timestamp\": \"2025-11-08T09:32:10.949573\"}, {\"session\": \"user001\", \"query\": \"Explain FastAPI briefly\", \"completeness\": 1.0, \"faithfulness\": 1.0, \"Retrieval Relevence\": 0.5390328764915466, \"timestamp\": \"2025-11-08T09:32:18.370347\"}]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Explain FastAPI briefly',\n",
              " 'context': 'FastAPI is a fast, modern framework for building web APIs in Python.\\nLangGraph lets you build agent workflows as graphs with memory and control flow.',\n",
              " 'answer': 'FastAPI is a fast, modern framework for building web APIs in Python. LangGraph lets you build agent workflows as graphs with memory and control flow.'}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}